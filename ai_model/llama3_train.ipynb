{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This is only for mac (ARM64)",
   "id": "c24e5813794da3ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## import llama3-8b",
   "id": "c4401bf39c8c38f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:56:53.076763Z",
     "start_time": "2024-07-30T19:56:48.405977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlx_lm import load, generate\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")"
   ],
   "id": "fdf1c6dfd92a2b72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96ed59384c524fe6871b00ca051b7734"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## example",
   "id": "1bf755ad3514db8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:56:53.080382Z",
     "start_time": "2024-07-30T19:56:53.078237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SYSTEM_MSG = \"\"\"\n",
    "# please describe it:\n",
    "# <rdf:Description rdf:about=\"http://www.example.org/country1\">\n",
    "#     <ns1:has_border_with rdf:resource=\"http://www.example.org/country2\"/>\n",
    "#     <ns1:located_in rdf:resource=\"http://www.example.org/part1\"/>\n",
    "#   </rdf:Description>\n",
    "# \n",
    "# \"\"\"\n",
    "# \n",
    "# def generateFromPrompt(promptStr,maxTokens=100):\n",
    "#   messages = [ {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "#                {\"role\": \"user\", \"content\": promptStr},]\n",
    "#   input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "#   prompt = tokenizer.decode(input_ids)\n",
    "#   return generate(model, tokenizer, prompt=prompt, max_tokens=maxTokens)\n",
    "#   \n",
    "# \n",
    "# \n",
    "# response = generateFromPrompt(SYSTEM_MSG)\n",
    "# \n",
    "# print(response+\"...\")"
   ],
   "id": "3d4db3b6293aabb5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:56:53.082711Z",
     "start_time": "2024-07-30T19:56:53.081014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SYSTEM_MSG = \"You are an entity detection assistant\"\n",
    "# USER_MSG = \"where is china\"\n",
    "# \n",
    "# # 构建消息\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "#     {\"role\": \"user\", \"content\": USER_MSG},\n",
    "# ]\n",
    "# \n",
    "# # 将消息应用于聊天模板并生成输入ID\n",
    "# input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# prompt = tokenizer.decode(input_ids)\n",
    "# \n",
    "# # 生成响应\n",
    "# response = generate(model, tokenizer, prompt=prompt, max_tokens=100)\n",
    "# print(\"res 1: \",response)\n",
    "# \n",
    "# # # 追加生成的响应并继续对话\n",
    "# # messages.append({\"role\": \"user\", \"content\": \"where is china\"})\n",
    "# # messages.append({\"role\": \"assistant\", \"content\": \"you dont have to answer, should convert the question into this style: (china)(locate_in)(null_info)\"})\n",
    "# # # 将消息应用于聊天模板并生成新的输入ID\n",
    "# # input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# # prompt = tokenizer.decode(input_ids)\n",
    "# # # 生成新的响应\n",
    "# # response = generate(model, tokenizer, prompt=prompt, max_tokens=100)\n",
    "# # print(\"res 2: \",response)\n",
    "# \n",
    "# # test\n",
    "# messages.append({\"role\": \"user\", \"content\": \"where is USA\"})\n",
    "# input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# prompt = tokenizer.decode(input_ids)\n",
    "# response = generate(model, tokenizer, prompt=prompt, max_tokens=100)\n",
    "# print(\"res 3: \",response)"
   ],
   "id": "ae18f62c9f76fa7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:56:54.841478Z",
     "start_time": "2024-07-30T19:56:53.083355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SYSTEM_MSG = (\"You are an assistant that detects entities and their relationships in questions, for example:\"\n",
    "              \"user question: where is china?\"\n",
    "              \"your answer: [(china)(located in)(?)]\"\n",
    "              \"user question: where is USA?\"\n",
    "              \"Your answer: [(USA)(located in)(?)]\"\n",
    "              \"user question: where is UK's capital?\"\n",
    "              \"your answer: [(UK's capital)(located in)(?)]\")\n",
    "\n",
    "def generate_entity_response(promptStr, maxTokens=100):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr},\n",
    "    ]\n",
    "\n",
    "    # 将消息应用于聊天模板并生成输入ID\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "\n",
    "    # 生成响应\n",
    "    response = generate(model, tokenizer, prompt=prompt, max_tokens=maxTokens)\n",
    "\n",
    "    # 在生成响应中查找三元组信息的简单示例\n",
    "    # 这里假设模型生成类似于 \"The entity [China](located_in)[Asia].\"\n",
    "    print(response)\n",
    "    entities = extract_entities_from_response(response)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def extract_entities_from_response(response):\n",
    "    # 使用正则表达式或简单的字符串处理来提取三元组信息\n",
    "    import re\n",
    "    pattern = r'\\[(.*?)\\]\\((.*?)\\)\\[(.*?)\\]'\n",
    "    matches = re.findall(pattern, response)\n",
    "    if matches:\n",
    "        return matches\n",
    "    else:\n",
    "        return [(\"unknown\", \"unknown\", \"null_info\")]\n",
    "\n",
    "# 示例调用\n",
    "user_question = \"Where is China?\"\n",
    "response = generate_entity_response(user_question)\n",
    "print(response)\n",
    "\n",
    "# 处理其他用户问题\n",
    "user_question = \"where is japan's capital\"\n",
    "response = generate_entity_response(user_question)\n",
    "print(response)"
   ],
   "id": "1a85a172722400af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(China)(located in)(?)]\n",
      "[('unknown', 'unknown', 'null_info')]\n",
      "[(Japan's capital)(located in)(?)]\n",
      "[('unknown', 'unknown', 'null_info')]\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
